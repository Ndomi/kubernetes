*** Download aws-cli v2 ***
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws --version

*** Install aws-cli v2 ***
sudo ./install --bin-dir /usr/bin/ --install-dir /usr/bin/aws-cli --update
aws --version

*** Create a AWS EKS user ***
aws configure


*** Install kubectl ***
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin
kubectl version --short --client

*** Install eksctl ***
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin/
eksctl version


*** Create eks cluster ***
eksctl create cluster --name dev-cluster --version 1.16 --region us-east-1 --nodegroup-name standard-workers --node-type t3.micro --nodes 3 --nodes-min 1 --nodes-max 4 --managed

                      OR
eksctl create cluster -f cluster.yaml

        --- Check cluster info ---
        eksctl get nodegroup  --cluster <cluster name>

Notes:
ASG: MIN SIZE 1
     MAX SIZE 4
     DESIRED CAPACITY 3


*** Scale nodes in a cluster ***
eksctl scale nodegroup --cluster=<cluster name> --nodes=5 --name=<nodegroup>

Since our desired capacity is 4, we will get an error:
 ++++++++++++++++++++++++++++++++++++++++++++++++
    Error: failed to scale nodegroup for cluster "dev-cluster", error: InvalidParameterException: desired capacity 5 can't be greater than max size 4
    {
      RespMetadata: {
        StatusCode: 400,
        RequestID: "f4a46c97-681d-4bb5-a0c3-7fb954f53f56"
      },
      ClusterName: "dev-cluster",
      Message_: "desired capacity 5 can't be greater than max size 4",
      NodegroupName: "standard-workers"
    }
++++++++++++++++++++++++++++++++++++++++++++++++


*** Delete a node group ***
eksctl delete nodegroup --include=<nodegroup> --approve
    # add --approve flag to avoid a dry-run

    # If the nodegroup was deployed using YAML
    eksctl delete nodegroup --config-file=<ABC.yml> --include=<nodegroup> --approve

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
Cluster Autoscaler
------------------
kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml


    # Ensure that the autoscaler does not evict it pods
     kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict="false" --overwrite

# Edit the Autoscaler and change the cluster version number to match your running Version(This is where you get the version Number "https://github.com/kubernetes/autoscaler/releases?after=vertical-pod-autoscaler-0.9.2")
# And insert you cluster name

kubectl -n kube-system edit deployment.apps/cluster-autoscaler

# Run the below to check
kubectl -n kube-system describe deployment

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
Implementing Autoscaling
------------------------

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Configure Cluster Monitoring
----------------------------

See cluster-monitoring.yml in Monitoring Folder
  - In this case, I named it  cluster-monitoring.yml

Enable EKS Cluster CloudWatch Monitoring
----------------------------
eksctl utils update-cluster-logging --config-file cluster-monitoring.yml --approve


Disable all EKS Cluster CloudWatch Monitoring
---------------------------------------------
eksctl utils update-cluster-logging --name=<cluster name> --disable-types all --approve

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Installing and deploying through HELM
--------------------------------------

curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash


Add the helm REPO
-----------------
helm repo add stable https://charts.helm.sh/stable

helm repo list
helm repo update

Doing a search
--------------
helm search repo

Install redis, using helm
-------------------------
helm install redis-test stable/redis

kubectl get pods

kubectl get pods

Clean update
------------
helm uninstall redis-test

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
RBAC for IAM users
++++++++++++++++++

Adding k8s cluster admin
------------------------
  - Add AIM user. Don't add any AWS Permissions

  Get ConfigMap
  -------------
    - kubectl  -n kube-system get cm
    
        - note the "aws-auth" ConfigMap

  Edit the "aws-auth" ConfigMap
  -----------------------------
  - kubectl  -n kube-system get cm aws-auth -o yaml > aws-auth-config.yml
      - Edit aws-auth-config.yml (See RBAC folder) 

  For more reference on RBAC, use the below link:
  -----------------------------------------------
    - https://kubernetes.io/docs/reference/access-authn-authz/rbac/

  Apply the "aws-auth-config.yml" changes
  ---------------------------------------
    - kubectl apply -f aws-auth-config.yml -n kube-system

  Check if the user is there:
  ---------------------------
    - kubectl -n kube-system describe cm aws-auth

  Edit ~/.aws/credentials with the users "Access key ID" and "Secret access key"
  ------------------------------------------------------------------------------

  Figure out which user is active
  -------------------------------
    - aws sts get-caller-identity

  Export our newly created user:
  ------------------------------
    - export AWS_PROFILE="clusteradmin"
            * This will change the user to "k8s-cluster-admin"(this was created in IAM)

  Check the access:
  -----------------
    - kubectl get nodes
    - kubectl get pod -n kube-system
    - kubectl run nginx --image=nginx --restart=Never


Adding k8s cluster read-only user
---------------------------------
    Create a namespace for production:
      - kubectl create namespace production

    Create the IAM user in AWS Console
    

    Specify the role for our user
      - vim role.yml (see RBAC folder)

    Bind user to the role
      - vim rolebinding.yml (rolebinding.yml)

    apply roles and role-binding
      - kubectl apply -f role.yml
      - kubectl apply -f rolebinding.yml

    Edit the "aws-auth" ConfigMap
    -----------------------------
      - kubectl  -n kube-system get cm aws-auth -o yaml > aws-auth-config.yml

      - Edit aws-auth-config.yml (See RBAC folder) 

      Edit ~/.aws/credentials with the users "Access key ID" and "Secret access key"
      ------------------------------------------------------------------------------

      Figure out which user is active
      -------------------------------
        - aws sts get-caller-identity

      Export our newly created user:
      ------------------------------
        - export AWS_PROFILE="prodviewer"
            * This will change the user to "k8s-cluster-admin"(this was created in IAM)

      Check the access:
      -----------------
        - kubectl  run nginx2 --image=nginx --restart=Never -n production